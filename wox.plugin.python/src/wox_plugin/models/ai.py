"""
Wox AI/LLM Models

This module provides models for interacting with AI/LLM services in Wox plugins.

Plugins can use AI models for tasks like text generation, summarization,
question answering, and more. The streaming API allows real-time display
of AI responses as they are generated.
"""

from enum import Enum
from typing import List, Callable, Optional, Any, Dict
import time
from dataclasses import dataclass, field
import json


class ConversationRole(str, Enum):
    """
    Enumeration of conversation participant roles.

    Used to identify who sent a message in a conversation history.
    Matches the Go backend's ConversationRole values exactly.
    """

    SYSTEM = "system"
    """
    Represents a system prompt message.

    System messages are used to set the behavior or personality of the AI.
    They are typically placed at the beginning of the conversation to
    provide context or instructions.

    System prompts are used to:
    - Define the AI's role or persona
    - Set behavioral guidelines
    - Provide context for the conversation

    Example:
        Conversation(
            role=ConversationRole.SYSTEM,
            text="You are a helpful assistant specializing in Python programming."
        )
    """

    USER = "user"
    """
    Represents a message sent by the human user.

    User messages contain the user's questions, requests, or input
    that the AI should respond to.
    """

    ASSISTANT = "assistant"
    """
    Represents a message generated by the AI assistant.

    Assistant messages contain the responses generated by the language model.
    These are typically the output from previous turns in a conversation.
    """

    TOOL = "tool"
    """
    Represents a message from a tool/function call.

    Tool messages contain the results of function calls made by the AI.
    When the AI requests to use a tool, the tool's response is passed
    back as a tool role message.

    This is part of the function calling feature where AI can interact
    with external tools/APIs.
    """


class ChatStreamDataType(str, Enum):
    """
    Enumeration of streaming status types for AI chat responses.

    These values indicate the current state of a streaming AI response,
    allowing plugins to handle updates, completion, and errors appropriately.
    """

    STREAMING = "streaming"
    """
    Currently streaming - new content is being received.

    When this status is received, the `data` field contains the
    accumulated content so far (all previous chunks combined).
    Plugins should update their display with each streaming message.

    The `reasoning` field may contain chain-of-thought reasoning
    from models that support it (e.g., DeepSeek-R1, OpenAI o1).
    """

    STREAMED = "streamed"
    """
    All content has been streamed.

    This status indicates that all data and tool calls have been streamed.
    If there are any tool calls, the status will transition to
    RUNNING_TOOL_CALL next. Otherwise, it will transition to FINISHED.
    """

    RUNNING_TOOL_CALL = "running_tool_call"
    """
    Tool calls are being executed.

    This status indicates that one or more tool calls are currently
    being executed. After all tool calls complete, the status will
    transition to FINISHED.

    During this phase, the `tool_calls` field contains information
    about the tool calls being executed.
    """

    FINISHED = "finished"
    """
    Stream completed successfully.

    This status indicates the AI has finished generating its response
    and all tool calls (if any) have completed. The `data` field
    contains the complete final response.

    Plugins should use this to finalize the display and clean up
    any loading indicators.

    Example:
        if stream_data.status == ChatStreamDataType.FINISHED:
            hide_loading_indicator()
            display_final_message(stream_data.data)
    """

    ERROR = "error"
    """
    An error occurred during streaming.

    This status indicates that the AI request failed due to an error
    such as network issues, API errors, or rate limiting. The `data`
    field may contain an error message.

    Example:
        if stream_data.status == ChatStreamDataType.ERROR:
            show_error(f"AI request failed: {stream_data.data}")
    """


@dataclass
class ToolCallInfo:
    """
    Information about a tool/function call made by the AI.

    When the AI decides to use a tool, it provides the tool name and
    arguments. This structure tracks the execution status of that call.

    Attributes:
        id: Unique identifier for this tool call
        name: Name of the tool/function being called
        arguments: Arguments to pass to the tool (key-value pairs)
        status: Current execution status of the tool call
        delta: Incremental content when tool call is streaming
        response: Response text from the tool execution
        start_timestamp: Unix timestamp in milliseconds when tool call started
        end_timestamp: Unix timestamp in milliseconds when tool call finished

    Example:
        tool_call = ToolCallInfo(
            id="call_123",
            name="search_web",
            arguments={"query": "Wox launcher"},
            status=ToolCallStatus.PENDING
        )
    """

    id: str
    """Unique identifier for this tool call."""

    name: str
    """Name of the tool/function being called."""

    arguments: Dict[str, Any] = field(default_factory=dict)
    """
    Arguments to pass to the tool.

    Key-value pairs of argument names and values that will be
    passed to the tool when executed.
    """

    status: str = "pending"
    """
    Current execution status.

    Valid values: "streaming", "pending", "running", "succeeded", "failed"
    """

    delta: str = ""
    """
    Incremental content when tool call is streaming.

    When the tool call response is streamed, this contains
    the most recent chunk of data.
    """

    response: str = ""
    """
    Response text from the tool execution.

    Contains the full response from the tool after execution completes.
    """

    start_timestamp: int = 0
    """Unix timestamp in milliseconds when tool call started."""

    end_timestamp: int = 0
    """Unix timestamp in milliseconds when tool call finished."""

    @classmethod
    def from_dict(cls, data: dict) -> "ToolCallInfo":
        """
        Create from dictionary.

        Args:
            data: Dictionary with tool call info

        Returns:
            A new ToolCallInfo instance
        """
        return cls(
            id=data.get("Id", ""),
            name=data.get("Name", ""),
            arguments=data.get("Arguments", {}),
            status=data.get("Status", "pending"),
            delta=data.get("Delta", ""),
            response=data.get("Response", ""),
            start_timestamp=data.get("StartTimestamp", 0),
            end_timestamp=data.get("EndTimestamp", 0),
        )


@dataclass
class ChatStreamData:
    """
    Chat stream data containing status, content, and optional reasoning.

    This class represents a chunk of streaming response data from an AI
    model. As the AI generates text, the callback receives multiple
    ChatStreamData objects with progressively more content.

    Attributes:
        status: The current streaming status
        data: The accumulated content generated so far (all chunks combined)
        reasoning: Chain-of-thought reasoning from models that support it
        tool_calls: List of tool calls being executed (if any)

    Example usage:
        def on_stream_data(stream_data: ChatStreamData):
            if stream_data.status == ChatStreamDataType.STREAMING:
                update_display(stream_data.data)
                if stream_data.reasoning:
                    show_reasoning(stream_data.reasoning)
            elif stream_data.status == ChatStreamDataType.RUNNING_TOOL_CALL:
                for tool_call in stream_data.tool_calls:
                    update_tool_status(tool_call.name, tool_call.status)
            elif stream_data.status == ChatStreamDataType.FINISHED:
                finalize_display(stream_data.data)
            elif stream_data.status == ChatStreamDataType.ERROR:
                show_error(stream_data.data)
    """

    status: ChatStreamDataType
    """
    The current streaming status.

    - STREAMING: Content is still being generated
    - STREAMED: All content has been streamed
    - RUNNING_TOOL_CALL: Tool calls are being executed
    - FINISHED: Generation completed successfully
    - ERROR: An error occurred
    """

    data: str
    """
    The accumulated content data.

    This field contains ALL the content generated so far, not just
    the new chunk. Each streaming message contains the complete
    response up to that point.

    For example, if the AI generates "Hello", then " World", then "!",
    the three callbacks will receive:
    1. status=STREAMING, data="Hello"
    2. status=STREAMING, data="Hello World"
    3. status=STREAMING, data="Hello World!"
    """

    reasoning: str = ""
    """
    Chain-of-thought reasoning content.

    Some AI models (like DeepSeek-R1, OpenAI o1) generate reasoning
    before producing the final answer. This field contains that
    reasoning process, which can be displayed separately to help
    users understand the AI's thought process.

    Not all models support reasoning. For models without this feature,
    the field will be empty.
    """

    tool_calls: List[ToolCallInfo] = field(default_factory=list)
    """
    List of tool calls being executed.

    When the AI decides to use tools, each tool call is represented
    by a ToolCallInfo object. The list includes all tool calls
    with their current status.

    Only populated when the AI makes tool calls.
    """

    @classmethod
    def from_dict(cls, data: dict) -> "ChatStreamData":
        """
        Create from dictionary.

        Args:
            data: Dictionary with streaming data

        Returns:
            A new ChatStreamData instance
        """
        tool_calls = []
        if "ToolCalls" in data:
            tool_calls = [ToolCallInfo.from_dict(tc) for tc in data.get("ToolCalls", [])]

        return cls(
            status=ChatStreamDataType(data.get("Status", "streaming")),
            data=data.get("Data", ""),
            reasoning=data.get("Reasoning", ""),
            tool_calls=tool_calls,
        )


# Type alias for the callback function that receives streaming AI responses.
#
# The callback is invoked multiple times as the AI generates its response:
# 1. Multiple times with status=STREAMING as content is generated
# 2. Once with status=STREAMED when content streaming is done
# 3. Once with status=RUNNING_TOOL_CALL if tools are being executed
# 4. Once with status=FINISHED when generation completes
# 5. Once with status=ERROR if an error occurs
#
# Example:
#     def my_callback(stream_data: ChatStreamData):
#         if stream_data.status == ChatStreamDataType.STREAMING:
#             print(f"Streaming: {stream_data.data}")
#         elif stream_data.status == ChatStreamDataType.FINISHED:
#             print(f"Complete: {stream_data.data}")
#         elif stream_data.status == ChatStreamDataType.ERROR:
#             print(f"Error: {stream_data.data}")
#
# Register with api.ai_chat_stream():
#     await api.ai_chat_stream(ctx, model, conversations, my_callback)
ChatStreamCallback = Callable[[ChatStreamData], None]


@dataclass
class AIModel:
    """
    AI model definition for specifying which LLM to use.

    Identifies a specific AI model and its provider. Wox supports
    multiple AI providers (OpenAI, Anthropic, Google, etc.) and
    each provider offers multiple models.

    Attributes:
        name: The model name/identifier (e.g., "gpt-4", "claude-3-opus")
        provider: The provider name (e.g., "openai", "anthropic")
        provider_alias: Optional alias for choosing specific provider config

    Example usage:
        model = AIModel(name="gpt-4", provider="openai")
        await api.ai_chat_stream(ctx, model, conversations, callback)

        # With provider alias (for multiple configs of same provider)
        model = AIModel(
            name="gpt-4",
            provider="openai",
            provider_alias="work-account"  # Use specific OpenAI config
        )
    """

    name: str
    """
    The model name/identifier.

    This should be a valid model name supported by the provider.
    Common examples:
    - OpenAI: "gpt-4", "gpt-3.5-turbo", "o1-preview"
    - Anthropic: "claude-3-opus", "claude-3-sonnet"
    - Google: "gemini-pro", "gemini-ultra"
    - DeepSeek: "deepseek-r1", "deepseek-chat"
    """

    provider: str
    """
    The AI service provider.

    The provider identifier as configured in Wox. This corresponds
    to the provider's internal ID in Wox's AI settings.

    Common providers:
    - "openai" - OpenAI (GPT models)
    - "anthropic" - Anthropic (Claude models)
    - "google" - Google (Gemini models)
    - "azure" - Azure OpenAI
    - "deepseek" - DeepSeek
    - Custom providers configured by the user
    """

    provider_alias: str = ""
    """
    Optional alias for choosing a specific provider configuration.

    When the user has configured multiple instances of the same provider
    (e.g., multiple OpenAI API keys), this alias selects which one to use.
    The alias should match the configuration name in Wox's AI settings.

    If empty, the default configuration for the provider is used.

    Example:
        # User has two OpenAI configs: "personal" and "work"
        model = AIModel(
            name="gpt-4",
            provider="openai",
            provider_alias="work"  # Use work account config
        )
    """

    def to_json(self) -> str:
        """
        Convert to JSON string with camelCase naming.

        Returns:
            JSON string representation
        """
        return json.dumps(
            {
                "Name": self.name,
                "Provider": self.provider,
                "ProviderAlias": self.provider_alias,
            }
        )

    @classmethod
    def from_json(cls, json_str: str) -> "AIModel":
        """
        Create from JSON string with camelCase naming.

        Args:
            json_str: JSON string with model data

        Returns:
            A new AIModel instance
        """
        data = json.loads(json_str)
        return cls(
            name=data.get("Name", ""),
            provider=data.get("Provider", ""),
            provider_alias=data.get("ProviderAlias", ""),
        )


@dataclass
class Conversation:
    """
    A single message in a conversation with an AI.

    Represents one message in a conversation history. Messages are
    accumulated and sent to the AI to provide context for multi-turn
    conversations.

    Attributes:
        role: Who sent the message (USER, ASSISTANT, SYSTEM, or TOOL)
        text: The text content of the message
        reasoning: Chain-of-thought reasoning (for assistant messages from reasoning models)
        images: Optional list of PNG image bytes for vision models
        tool_call_info: Tool call information (for TOOL role messages)
        timestamp: Unix timestamp in milliseconds when message was created

    Example usage:
        # Create a conversation history
        conversations = [
            Conversation.new_user_message("What is Python?"),
            Conversation.new_assistant_message("Python is a programming language..."),
            Conversation.new_user_message("Show me an example"),
        ]

        # Send to AI
        await api.ai_chat_stream(ctx, model, conversations, callback)
    """

    role: ConversationRole
    """
    The role of the message sender.

    Valid values:
    - USER: Messages from the human
    - ASSISTANT: Messages from the AI assistant
    - SYSTEM: System prompts that set AI behavior
    - TOOL: Results from tool/function calls
    """

    text: str
    """
    The text content of the message.

    For user messages, this is the question or request.
    For assistant messages, this is the response.
    For system messages, this is the system prompt.
    For tool messages, this is the tool result.
    """

    reasoning: str = ""
    """
    Chain-of-thought reasoning content.

    Some AI models (like DeepSeek-R1, OpenAI o1, qwen3) generate
    reasoning before producing the final answer. This field contains
    that reasoning process.

    Only applicable for assistant messages from models that support
    reasoning. Empty for other message types and models.
    """

    images: List[bytes] = field(default_factory=list)
    """
    Optional list of PNG format image data.

    Used for vision-capable models that can process images alongside text.
    Each item should be raw PNG bytes.

    Example:
        with open("image.png", "rb") as f:
            image_data = f.read()

        msg = Conversation.new_user_message(
            "What's in this image?",
            images=[image_data]
        )
    """

    tool_call_info: Optional[ToolCallInfo] = None
    """
    Tool call information for TOOL role messages.

    When the message represents a tool call result, this field
    contains information about which tool was called and its response.
    """

    timestamp: int = field(default_factory=lambda: int(time.time() * 1000))
    """
    Unix timestamp in milliseconds when the message was created.

    Automatically set to the current time when the Conversation is created.
    """

    def to_json(self) -> str:
        """
        Convert to JSON string with camelCase naming.

        Images are converted to hex strings for JSON serialization.

        Returns:
            JSON string representation
        """
        data = {
            "Role": self.role,
            "Text": self.text,
            "Images": [image.hex() for image in self.images] if self.images else [],
            "Timestamp": self.timestamp,
        }

        if self.reasoning:
            data["Reasoning"] = self.reasoning

        if self.tool_call_info:
            data["ToolCallInfo"] = {
                "Id": self.tool_call_info.id,
                "Name": self.tool_call_info.name,
                "Arguments": self.tool_call_info.arguments,
                "Status": self.tool_call_info.status,
                "Delta": self.tool_call_info.delta,
                "Response": self.tool_call_info.response,
                "StartTimestamp": self.tool_call_info.start_timestamp,
                "EndTimestamp": self.tool_call_info.end_timestamp,
            }

        return json.dumps(data)

    @classmethod
    def from_json(cls, json_str: str) -> "Conversation":
        """
        Create from JSON string with camelCase naming.

        Args:
            json_str: JSON string containing conversation data

        Returns:
            A new Conversation instance
        """
        data = json.loads(json_str)

        if not data.get("Role"):
            data["Role"] = ConversationRole.USER

        tool_call_info = None
        if "ToolCallInfo" in data and data["ToolCallInfo"]:
            tci = data["ToolCallInfo"]
            tool_call_info = ToolCallInfo(
                id=tci.get("Id", ""),
                name=tci.get("Name", ""),
                arguments=tci.get("Arguments", {}),
                status=tci.get("Status", "pending"),
                delta=tci.get("Delta", ""),
                response=tci.get("Response", ""),
                start_timestamp=tci.get("StartTimestamp", 0),
                end_timestamp=tci.get("EndTimestamp", 0),
            )

        return cls(
            role=ConversationRole(data.get("Role")),
            text=data.get("Text", ""),
            reasoning=data.get("Reasoning", ""),
            images=[bytes.fromhex(img) for img in data.get("Images", [])] if data.get("Images") else [],
            tool_call_info=tool_call_info,
            timestamp=data.get("Timestamp", int(time.time() * 1000)),
        )

    @classmethod
    def new_user_message(cls, text: str, images: Optional[List[bytes]] = None) -> "Conversation":
        """
        Create a user message.

        Factory method to create a Conversation with role=USER.

        Args:
            text: The user's message text
            images: Optional list of PNG image bytes for vision models

        Returns:
            A new Conversation instance with role=USER

        Example:
            msg = Conversation.new_user_message("Hello, how are you?")
        """
        return cls(
            role=ConversationRole.USER,
            text=text,
            images=images if images is not None else [],
        )

    @classmethod
    def new_assistant_message(cls, text: str, reasoning: str = "") -> "Conversation":
        """
        Create an assistant message.

        Factory method to create a Conversation with role=ASSISTANT.

        Args:
            text: The assistant's response text
            reasoning: Optional reasoning content from reasoning models

        Returns:
            A new Conversation instance with role=ASSISTANT

        Example:
            msg = Conversation.new_assistant_message("I'm doing well, thank you!")

            # With reasoning (for DeepSeek-R1, OpenAI o1, etc.)
            msg = Conversation.new_assistant_message(
                text="The answer is 42.",
                reasoning="Let me think about this step by step..."
            )
        """
        return cls(
            role=ConversationRole.ASSISTANT,
            text=text,
            reasoning=reasoning,
        )

    @classmethod
    def new_system_message(cls, text: str) -> "Conversation":
        """
        Create a system message.

        Factory method to create a Conversation with role=SYSTEM.
        System messages set the behavior or personality of the AI.

        Args:
            text: The system prompt text

        Returns:
            A new Conversation instance with role=SYSTEM

        Example:
            msg = Conversation.new_system_message(
                "You are a helpful assistant specializing in Python programming."
            )
        """
        return cls(
            role=ConversationRole.SYSTEM,
            text=text,
        )

    @classmethod
    def new_tool_message(cls, text: str, tool_call_info: ToolCallInfo) -> "Conversation":
        """
        Create a tool message.

        Factory method to create a Conversation with role=TOOL.
        Tool messages contain the results of function calls.

        Args:
            text: The tool's response/result
            tool_call_info: Information about the tool call

        Returns:
            A new Conversation instance with role=TOOL

        Example:
            tool_info = ToolCallInfo(
                id="call_123",
                name="search",
                arguments={"query": "python"},
                status="succeeded"
            )
            msg = Conversation.new_tool_message(
                text="Search results: [...]",
                tool_call_info=tool_info
            )
        """
        return cls(
            role=ConversationRole.TOOL,
            text=text,
            tool_call_info=tool_call_info,
        )
